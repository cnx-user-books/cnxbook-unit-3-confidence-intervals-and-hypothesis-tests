<document xmlns="http://cnx.rice.edu/cnxml" xmlns:m="http://www.w3.org/1998/Math/MathML">
  <title>Distribution Needed for Hypothesis Testing</title>
  <metadata xmlns:md="http://cnx.rice.edu/mdml">
  <md:content-id>m65305</md:content-id>
  <md:title>Distribution Needed for Hypothesis Testing</md:title>
  <md:abstract/>
  <md:uuid>8bc32a80-19b1-44ad-8502-e7ccd2353368</md:uuid>
</metadata>
<featured-links>
  <!-- WARNING! The 'featured-links' section is read only. Do not edit below.
       Changes to the links section in the source will not be saved. -->
    <link-group type="example">
      <link url="https://legacy.cnx.org/content/m65027/latest/#uid45" strength="3">Figure 6</link>
    </link-group>
  <!-- WARNING! The 'featured-links' section is read only. Do not edit above.
       Changes to the links section in the source will not be saved. -->
</featured-links>
<content>
  <para id="fs-idm27402752">In chapter 6, we discussed sampling distributions, which are used for hypothesis testing. We will perform hypotheses tests of a population mean using two particular sampling distributions: a <term class="no-emphasis">normal distribution</term> or a <term class="no-emphasis">Student's <emphasis effect="italics">t</emphasis>-distribution</term>.  We will perform hypothesis tests of a population proportion using a normal sampling distribution that has been approximated from a binomial situation. </para><section id="eip-578"><title>Central Limit Theorem Revisited</title><para id="eip-490">When you perform a <emphasis>hypothesis test of a single population mean <emphasis effect="italics">μ</emphasis></emphasis> using a
normal distribution (often called a <emphasis effect="italics">z</emphasis>-test), you take a large random sample from the population. When working with large samples, you should recall from chapter 6 that Central Limit Theorem says that the sampling distribution of means will be approximately normal even if the population from whence the sample came is not. For this reason we can perform hypothesis tests using large samples and the normal distribution regardless of the shape of the parent population.</para><para id="eip-906">Many statisticians prefer to use a t-distribution if the population standard deviation is unknown, even if the samples are large. The reasoning behind this is that using the sample standard deviation in place of the unknown population standard deviation adds an extra degree of potential error that can only be accounted for by using a t- distribution. 

However, as noted in the previous chapter, it is common practice to use the normal (Z-based) sampling distribution when working with large samples. Specifically, when n&gt;40, we will use the standard normal(z-based)distribution to conduct a hypothesis test..</para><para id="eip-324">When working with small samples, we will perform a <term>hypothesis test</term> <emphasis>of a single population mean <emphasis effect="italics">μ</emphasis></emphasis> using a
<term>Student's <emphasis effect="italics">t</emphasis>-distribution</term> (often called a t-test). There are fundamental assumptions that need to be met in order for the test to be considered valid. Most importantly, since Central Limit Theorem does not apply to small samples, we have no guarantee the the sampling distribution will be normally shaped. For this reason, we can only perform means tests with small samples when we know the population is normally distributed. </para><note id="eip-259">Please see Figure 6 in the previous chapter for further insight into how to determine which sampling distribution is appropriate when conducting a hypotheses test of a population mean.</note><para id="element-395">When you perform a <emphasis>hypothesis test of a single population proportion <emphasis effect="italics">p</emphasis></emphasis>, you
  take a random sample from the population. You must meet the conditions for a <term>binomial distribution</term> which are: there are a certain number <emphasis effect="italics">n</emphasis> of independent trials, the outcomes of any trial are success or failure, and each trial has the same probability of a success <emphasis effect="italics">p</emphasis>. The Central Limit Theorem says the shape of the binomial distribution will approximate the shape of the normal distribution if the sample is sufficiently large. To ensure this, the quantities <emphasis effect="italics">np</emphasis> and <emphasis effect="italics">nq</emphasis> must both be greater than five (<emphasis effect="italics">np</emphasis> &gt; 5 and <emphasis effect="italics">nq</emphasis> &gt; 5). Then the binomial distribution of a sample (estimated) proportion can be approximated by the normal distribution with <emphasis effect="italics">μ</emphasis> = <emphasis effect="italics">p</emphasis> and <m:math>
    <m:mrow>
      <m:mi>σ</m:mi><m:mo>=</m:mo><m:msqrt>
        <m:mrow>
          <m:mfrac>
            <m:mrow>
              <m:mi>p</m:mi><m:mi>q</m:mi>
            </m:mrow>
            <m:mi>n</m:mi>
          </m:mfrac>
          
        </m:mrow>
      </m:msqrt>
      
    </m:mrow>
  </m:math>.

Remember that <emphasis effect="italics">q</emphasis> = 1 – <emphasis effect="italics">p</emphasis>.</para></section><section id="eip-162"><title>Large Sample Hypothesis Test for the Mean</title><para id="eip-328">Going back to the standardizing formula we can derive the <term>test statistic</term> for testing hypotheses concerning means. We have already worked with the formula below when introduced to sampling distributions in Chapter 6. You should, however, notice one small difference. When we perform hypothesis tests, we don't know the population mean; we simply have a claim or belief about the mean, which may or may not be true. Because the mean is hypothesized rather than known, we use a slightly different symbol in the equation, μ<sub>0</sub>, as seen below.
</para><equation id="eip-255"><m:math>
<m:msub><m:mi>Z</m:mi><m:mi>c</m:mi></m:msub>
<m:mo>=</m:mo>
<m:mfrac>
<m:mrow>
<m:mover><m:mi>x</m:mi><m:mo>–</m:mo></m:mover><m:mo>-</m:mo><m:msub><m:mi>μ</m:mi><m:mn>0</m:mn></m:msub></m:mrow>
<m:mrow>
<m:mi>σ</m:mi><m:mo>/</m:mo><m:msqrt><m:mi>n</m:mi></m:msqrt></m:mrow></m:mfrac>
</m:math></equation><para id="eip-233">This calculated Z is nothing more than the number of standard deviations that the sample mean is from the <emphasis>hypothesized</emphasis> population mean. If the sample mean falls "too many" standard deviations from the hypothesized mean we conclude that the <emphasis>sample</emphasis> mean is <emphasis>unlikely</emphasis> to have come from a distribution centred around the hypothesized mean.</para><para id="eip-315">So how do we know if a sample mean can be considered to have fallen "too many" standard deviations away from a hypothesized mean? Obviously, we can't simply make this decision arbitrarily. Thankfully, we have already been introduced this concept when we examined confidence intervals in the previous chapter. Just as we predetermine our level of confidence before we compute an estimate of a population parameter, so too must we predetermine how strong we need our sample evidence to be (i.e. how many standard deviations away from the hypothesized population parameter it must lie) before we would be confident in rejecting the null hypothesis. This predetermined level in hypothesis testing is called the level of significance, and it is simply 1- the level of confidence. The level of significance is denoted as alpha (α).  </para><para id="eip-487">This level of significance delineates a set number of standard deviations between evidence that would be considered unlikely and evidence that would be considered not unlikely under the assumption that the null hypothesis is true. By way of example, say we set our level of significance at 5%. The corresponding Z score for a 5% level of significance is 1.645. This means that if our sample mean falls more than 1.645 standard deviations away from the hypothesized middle of the distribution (i.e. the null hypothesis), we can conclude the sample evidence is strong enough to be considered an unlikely event and we can therefore reject the null hypothesis.
</para><para id="eip-934">Before proceeding further, it's worth reviewing this notion of a significance level from another perspective. The significance level can be thought of as the allowable amount of error in our test. Just as a 95% confidence level will produce an incorrect estimate 5% of the time, so will our hypothesis test with a level of significance set at 5%, produce an incorrect conclusion 5% of the time, at least theoretically. When we set the significance level at, say 5%, we are essentially saying that on our sampling distribution any sample mean that falls into the top (or bottom) 5% of the tail would be considered strong evidence against the null hypothesis. This does not mean the evidence is perfect, however. There is certainly the possibility that a sample mean that falls into the top (or bottom ) 5% of the tail could have come from a population in which the null hypothesis is true. Indeed that possibility is actually 5%. But 5% is a pretty small number, which is why we would say the observance of such a sample mean must be considered an unlikely--but not impossible-- event.  </para></section><section id="eip-234"><title>Small Sample Hypothesis Tests for the Mean</title><para id="eip-616">Because the samples are small and we don't know the population standard deviation, we must use a Student t-distribution rather than a Z distribution to perform our tests. The new standardizing formula below will be used to compute how many standard deviations our sample mean falls from the hypothesized middle of the t-distribution.
</para><equation id="eip-474"><m:math>
      <m:msub><m:mi>t</m:mi><m:mi>c</m:mi></m:msub><m:mo>=</m:mo>
      <m:mfrac><m:mrow><m:mover><m:mi>X</m:mi><m:mo>–</m:mo></m:mover>
      <m:mo>-</m:mo><m:msub><m:mi>μ</m:mi><m:mn>0</m:mn></m:msub></m:mrow>
      <m:mrow><m:mi>s</m:mi><m:mo>/</m:mo><m:msqrt><m:mi>n</m:mi></m:msqrt>
      </m:mrow></m:mfrac>
     </m:math></equation></section><section id="eip-821"><title>Large Sample Tests for the Proportion</title><para id="eip-187">When conducting a hypothesis test on a proportion, we can use a Z-based test so long as the sample is sufficiently large. A sample is considered large if n<m:math overflow="scroll"><m:mover accent="true"><m:mi>p</m:mi><m:mo>^</m:mo></m:mover></m:math> and n(1-<m:math overflow="scroll"><m:mover accent="true"><m:mi>p</m:mi><m:mo>^</m:mo></m:mover></m:math>) both exceed 5. Even though we will perform a Z-based test, because we are working with proportions, the standardizing formula is quite different. In the numerator, the hypothesized proportion is subtracted from the observed sample proportion. In the denominator, the standard error is calculated by first multiplying the hypothesized proportion by 1 - the hypothesized proportion; then by dividing the result; and finally taking the square root of that result.
</para><para id="eip-505">Z* = <m:math overflow="scroll"><m:mover accent="true"><m:mi>p</m:mi><m:mo>^</m:mo></m:mover></m:math> -  <m:math overflow="scroll"><m:mi>πo</m:mi></m:math>/<m:math overflow="scroll"><m:mstyle scriptlevel="0" displaystyle="true"><m:msqrt><m:mfrac><m:mrow><m:mi>πo</m:mi><m:mo>(</m:mo><m:mn>1</m:mn><m:mo>-</m:mo><m:mi>πo</m:mi><m:mo>)</m:mo></m:mrow><m:mi>n</m:mi></m:mfrac></m:msqrt></m:mstyle></m:math>.</para></section><section id="eip-952" class="summary">
  <title>Chapter Review</title>
  <para id="eip-948">In order for a hypothesis test’s results to be generalized to a population, certain requirements must be satisfied. </para>
  <para id="eip-996">When testing for a single population mean:</para>
  <list id="eip-idp106869104" mark-suffix="." list-type="enumerated" number-style="arabic"><item>A Student's <emphasis effect="italics">t</emphasis>-test should be used if the data come from a small, random sample and the population is approximately normally distributed.</item>
    <item>The normal z-test can be used if the data come from a large, random sample. The population does not need to be normally distributed.</item>
  </list><para id="fs-idp12045392">When testing a single population proportion use a normal test for a single population proportion if the data comes from a random sample, fit the requirements for a binomial distribution, and the mean number of success and the mean number of failures satisfy the conditions: <emphasis effect="italics">np</emphasis> &gt; 5 and <emphasis effect="italics">nq</emphasis> &gt; <emphasis effect="italics">n</emphasis> where <emphasis effect="italics">n</emphasis> is the sample size, <emphasis effect="italics">p</emphasis> is the probability of a success, and <emphasis effect="italics">q</emphasis> is the probability of a failure.</para></section>


  <section id="eip-496" class="practice">
  <exercise id="eip-176"><problem id="eip-548">
  <para id="eip-434">Which two distributions can you use in hypothesis testing for the mean in this chapter?</para></problem>

<solution id="eip-482">
  <para id="eip-665">A normal distribution or a Student’s <emphasis effect="italics">t</emphasis>-distribution</para></solution>
</exercise><exercise id="eip-579">
    <problem id="eip-177">
  <para id="eip-756">Which distribution do you use when the sample size is small, the standard deviation is not known and you are testing one population mean? </para></problem>

<solution id="eip-957">
  <para id="eip-853">Use a Student’s <emphasis effect="italics">t</emphasis>-distribution</para></solution>
  </exercise>
  <exercise id="eip-917">
    <problem id="eip-265">
  <para id="eip-995">A population has a mean is 25 and a standard deviation of five. The sample mean is 24, and the sample size is 108. What distribution should you use to perform a hypothesis test?</para>
</problem>

<solution id="eip-953">
  <para id="eip-267">a normal distribution for a single population mean</para>
</solution>
  </exercise>
  <exercise id="eip-637"><problem id="eip-74">
  <para id="eip-239">You are performing a hypothesis test of a single population mean using a Student’s <emphasis effect="italics">t</emphasis>-distribution. What must you assume about the distribution of the data?</para></problem>

<solution id="eip-520">
  <para id="eip-20">It must be approximately normally distributed.</para>
</solution>
</exercise>
  <exercise id="eip-254">
    <problem id="eip-543">
      <para id="eip-372">You are performing a hypothesis test of a single population proportion. What must be true about the quantities of n<m:math overflow="scroll"><m:mover accent="true"><m:mi>p</m:mi><m:mo>^</m:mo></m:mover></m:math> and n(1-<m:math overflow="scroll"><m:mover accent="true"><m:mi>p</m:mi><m:mo>^</m:mo></m:mover></m:math>)</para></problem>

<solution id="eip-509">
  <para id="eip-285">They must both be greater than five.</para>
</solution>
  </exercise><exercise id="fs-idp17688688">
    <problem id="eip-577">
      <para id="eip-129">You are performing a hypothesis test of a single population proportion. The data come from which distribution?</para>
</problem>

<solution id="eip-979">
  <para id="eip-676">binomial distribution<newline/></para>
</solution>
</exercise>

</section>

<section id="fs-idm173512608" class="free-response">
<title>Homework</title>

<exercise id="fs-idm140519760"><problem id="fs-idm140519632"><para id="fs-idp12923264">It is believed that Medicine Hat Community College (MHCC) Intermediate Accounting students get more than seven hours of sleep per night, on average. A survey of 22 MHCC Intermediate Accounting students generated a mean of 7.24 hours with a standard deviation of 1.93 hours. At a level of significance of 5%, do MHCC Intermediate Accounting students get more than seven hours of sleep per night, on average? The distribution to be used for this test is <m:math>
    <m:mover accent="true">
      <m:mi>X</m:mi>
      <m:mo>–</m:mo>
    </m:mover>
    
  </m:math> ~ ________________</para><list id="fs-idm141858480" list-type="enumerated" number-style="lower-alpha"><item><m:math>
        <m:mrow>
          <m:mi>Z</m:mi><m:mo stretchy="false">(</m:mo><m:mn>7.24</m:mn><m:mo>,</m:mo><m:mfrac>
            <m:mrow>
              <m:mn>1.93</m:mn>
            </m:mrow>
            <m:mrow>
              <m:msqrt>
                <m:mrow>
                  <m:mn>22</m:mn>
                </m:mrow>
              </m:msqrt>
              
            </m:mrow>
          </m:mfrac>
          <m:mo stretchy="false">)</m:mo>
        </m:mrow>
      </m:math></item>
      <item><m:math>
        <m:mrow>
          <m:mi>Z</m:mi><m:mo stretchy="false">(</m:mo><m:mn>7.24</m:mn><m:mo>,</m:mo><m:mn>1.93</m:mn><m:mo stretchy="false">)</m:mo>
        </m:mrow>
      </m:math></item>
      <item><emphasis effect="italics">t</emphasis><sub>22 df</sub></item>
      <item><emphasis effect="italics">t</emphasis><sub>21 df</sub></item>
    </list></problem>

<solution id="fs-idp8543664">
<para id="fs-idm76617584">d</para>
</solution>
</exercise></section>

</content>

<glossary>
<definition id="bidist">
    <term>Binomial Distribution</term>
    <meaning id="fs-idm119055264">a discrete random variable (RV) that arises from Bernoulli trials. There are a fixed number, <emphasis effect="italics">n</emphasis>, of independent trials. “Independent” means that the result of any trial (for example, trial 1) does not affect the results of the following trials, and all trials are conducted under the same conditions. Under these circumstances the binomial RV Χ is defined as the number of successes in <emphasis effect="italics">n</emphasis> trials. The notation is: <emphasis effect="italics">X ~ B(n, p)</emphasis> <emphasis effect="italics">μ</emphasis> = <emphasis effect="italics">np</emphasis>  and the standard deviation is <m:math>
 <m:mrow>
  <m:mi>σ</m:mi><m:mo>=</m:mo><m:mo> </m:mo><m:msqrt>
   <m:mrow>
    <m:mi>n</m:mi><m:mi>p</m:mi><m:mi>q</m:mi>
   </m:mrow>
  </m:msqrt>
  
 </m:mrow>
    </m:math>. The probability of exactly <emphasis effect="italics">x</emphasis> successes in <emphasis effect="italics">n</emphasis> trials is <m:math>
      <m:mrow>
        <m:mi>P</m:mi><m:mo stretchy="false">(</m:mo><m:mi>X</m:mi><m:mo>=</m:mo><m:mi>x</m:mi><m:mo stretchy="false">)</m:mo><m:mo>=</m:mo><m:mrow><m:mo>(</m:mo>
          <m:mrow>
            <m:mtable>
              <m:mtr>
                <m:mtd>
                  <m:mi>n</m:mi>
                </m:mtd>
              </m:mtr>
              <m:mtr>
                <m:mtd>
                  <m:mi>x</m:mi>
                </m:mtd>
              </m:mtr>
              
            </m:mtable>
          </m:mrow>
          <m:mo>)</m:mo></m:mrow><m:msup>
            <m:mi>p</m:mi>
            <m:mi>x</m:mi>
          </m:msup>
        <m:msup>
          <m:mi>q</m:mi>
          <m:mrow>
            <m:mi>n</m:mi><m:mo>−</m:mo><m:mi>x</m:mi>
          </m:mrow>
        </m:msup>
        
      </m:mrow>
    </m:math>.
</meaning>
  </definition>


<definition id="normdist">
    <term>Normal Distribution</term>
  <meaning id="id42733014">a continuous random variable (RV) with pdf <m:math>
    <m:mrow>
      <m:mi>f</m:mi><m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mo stretchy="false">)</m:mo><m:mo>=</m:mo><m:mo> </m:mo><m:mfrac>
        <m:mn>1</m:mn>
        <m:mrow>
          <m:mi>σ</m:mi><m:msqrt>
            <m:mrow>
              <m:mn>2</m:mn><m:mi>π</m:mi>
            </m:mrow>
          </m:msqrt>
          
        </m:mrow>
      </m:mfrac>
      <m:msup>
        <m:mi>e</m:mi>
        <m:mrow>
          <m:mfrac>
            <m:mrow>
              <m:mo>−</m:mo><m:msup>
                <m:mrow>
                  <m:mo stretchy="false">(</m:mo><m:mi>x</m:mi><m:mo>−</m:mo><m:mi>μ</m:mi><m:mo stretchy="false">)</m:mo>
                </m:mrow>
                <m:mn>2</m:mn>
              </m:msup>
              
            </m:mrow>
            <m:mrow>
              <m:mn>2</m:mn><m:msup>
                <m:mi>σ</m:mi>
                <m:mn>2</m:mn>
              </m:msup>
              
            </m:mrow>
          </m:mfrac>
          
        </m:mrow>
      </m:msup>
      
    </m:mrow>
  </m:math>, where <emphasis effect="italics">μ</emphasis> is the mean of the distribution, and <emphasis effect="italics">σ</emphasis> is the standard deviation, notation: <emphasis effect="italics">X ~ N</emphasis>(<emphasis effect="italics">μ</emphasis>, <emphasis effect="italics">σ</emphasis>). If <emphasis effect="italics">μ</emphasis> = 0 and <emphasis effect="italics">σ</emphasis> = 1, the RV is called <emphasis>the standard normal distribution</emphasis>.</meaning>
  </definition>

  <definition id="stddev">
    <term>Standard Deviation</term>
    <meaning id="id20302532">a number that is equal to the square root of the variance and measures how far data values are from their mean; notation: <emphasis effect="italics">s</emphasis> for sample standard deviation and <emphasis effect="italics">σ</emphasis> for population standard deviation.</meaning>
  </definition>

<definition id="studenttdist">
    <term>Student's <emphasis effect="italics">t</emphasis>-Distribution</term>
    <meaning id="id8759760">investigated and reported by William S. Gossett in 1908 and published under the pseudonym Student. The major characteristics of the random variable (RV) are: 
<list id="tdist1" list-type="bulleted">
  <item>It is continuous and assumes any real values.</item>
  <item>The pdf is symmetrical about its mean of zero. However, it is more spread out and flatter at the apex than the normal distribution.</item>
  <item>It approaches the standard normal distribution as <emphasis effect="italics">n</emphasis> gets larger.</item>
  <item>There is a "family" of t distributions: every representative of the family is completely defined by the number of degrees of freedom which is one less than the number of data items.</item></list></meaning>
  </definition>

<definition id="teststatistic">
<term>Test Statistic</term> <meaning id="fs-id1165813764231">The formula that counts the number of standard deviations on the relevant distribution  that estimated parameter is away from the hypothesized value.</meaning></definition>

<definition id="criticalvalue">
<term>Critical Value</term> <meaning id="fs-id1167884651205">The <emphasis effect="italics">t</emphasis> or <emphasis effect="italics">Z</emphasis> value set by the researcher that measures the probability of a Type I error, α.</meaning></definition>


</glossary>
</document>